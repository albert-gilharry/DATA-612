{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA 612 Final Project: Instacart Recommendations Using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instacart in one of the most popular grocery ordering and delivery apps. After browsing and selecting products through the app, personal shoppers review orders and do in-store shopping and deliveries for customers. Due to the vast majority of available retailers, inventory size, and diverse customer base, it is very important for Instacart to assist its customers in navigating  the wealth of available options by providing useful personalized recommendations. It is in this light that I will build a recommender system using the Instacart Online Grocery Shopping Dataset 2017 that they have made publicly available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit ratings for products were not provided, therefore they recommender will be based on **implicit ratings** based on the number of times the product was purchased by each user. A collaborative filtering approach will be used because products will be recommended based on the interactions of other users.  The main algorithm that will be used is **Alternative Least Squares (ALS)** because it is a proven method for decomposing the very large user-item matrix into lower dimensional user factors and item factors and also because it has been optimized for the distrubutive in-memory processing of the Spark framework. The goal is to  minimizing the **RMSE** by tuning different parameters of the ALS function. After suitable parameters have been determined, the model will be built on an **EC2** instance and a web application using **Python's Flask** framework will built to demonstrate it's use in an online setting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is anonymized and contains a sample of over **3 million** grocery orders from more than 200,000 Instacart users. The data is a relational database separated into 6 tables describing customers' orders over time. For each user, between 4 and 100 of their orders are provided, with the sequence of products purchased in each order. \n",
    "\n",
    "The dataset is provided as-is for non-commercial use, and may be accessed from https://www.instacart.com/datasets/grocery-shopping-2017, or from the Kaggle competition \"Instacart Market Basket Analysis\" at https://www.kaggle.com/c/instacart-market-basket-analysis/overview. The data will be cited as specified.\n",
    "\n",
    "We will start by loading the data into Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to Spark and Loading Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is quite large so we intialize the Spark session with a healthy amount of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import  SparkSession\n",
    "from pyspark.sql.types import StructType, IntegerType, StructField\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '20G'),\n",
    "                                        ('spark.app.name', 'Spark Updated Conf'), \n",
    "                                        ('spark.executor.cores', '4'), \n",
    "                                        ('spark.cores.max', '4'), \n",
    "                                        ('spark.driver.memory','20G'),\n",
    "                                        ('spark.executor.memoryOverhead', '20G')])\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here will load the data files and persist them so that we may access them in future without havig to re-load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\taisles_df = spark.read.parquet(\"aisles.parquet\")\n",
    "\tdepartments_df = spark.read.parquet(\"departments.parquet\")\n",
    "\torder_products_df = spark.read.parquet(\"order_products.parquet\")\n",
    "\torders_df = spark.read.parquet(\"orders.parquet\")\n",
    "\tproducts_df = spark.read.parquet(\"products.parquet\")\n",
    "\n",
    "except:\n",
    "\t# if not, load data files\n",
    "\taisles_df = spark.read.format('csv').options(header='true', inferSchema='true').load('data/aisles.csv')\n",
    "\tdepartments_df = spark.read.format('csv').options(header='true', inferSchema='true').load('data/departments.csv')\n",
    "\torder_products = spark.read.format('csv').options(header='true', inferSchema='true').load('data/order_products__prior.csv')\n",
    "\torder_products_df = order_products.union(spark.read.format('csv').options(header='true', inferSchema='true').load('data/order_products__train.csv'))\n",
    "\torders_df = spark.read.format('csv').options(header='true', inferSchema='true').load('data/orders.csv')\n",
    "\tproducts_df = spark.read.format('csv').options(header='true', inferSchema='true').load('data/products.csv')\n",
    "\t# persist data to Spark for future access\n",
    "\taisles_df.write.parquet(\"aisles.parquet\", mode = \"overwrite\" )\n",
    "\tdepartments_df.write.parquet(\"departments.parquet\")\n",
    "\torder_products_df.write.parquet(\"order_products.parquet\")\n",
    "\torders_df.write.parquet(\"orders.parquet\")\n",
    "\tproducts_df.write.parquet(\"products.parquet\")\n",
    "    \n",
    "users = orders_df.groupBy(\"user_id\").count()\n",
    "users = users.drop(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of users are 206209\n",
      "The number of products are 49688\n",
      "The number of orders are 3421083\n",
      "The number of aisles are 134\n",
      "The number of departments are 21\n",
      "The top product is Banana\n"
     ]
    }
   ],
   "source": [
    "top_products = order_products_df.groupBy(\"product_id\").count()\n",
    "top_products = top_products.sort(\"count\", ascending=False)\n",
    "top_products = top_products.limit(1).join(products_df, top_products.product_id == products_df.product_id)\n",
    "top_product=top_products.collect()[0]['product_name']\n",
    "\n",
    "print(\"The number of users are \" + str(users.count()) )\n",
    "print(\"The number of products are \" + str(products_df.count()) )\n",
    "print(\"The number of orders are \" + str(orders_df.count()) )\n",
    "print(\"The number of aisles are \" + str(aisles_df.count()) )\n",
    "print(\"The number of departments are \" + str(departments_df.count()) )\n",
    "print(\"The top product is \" + str(top_product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there over **200,000 users, almost 50,000 products from a total of 134 asles, and over 3.4 million orders**. The top rated product is **banana** based on number of orders. Now lets calculate the implicit ratings for user and product interactions based on the number of purchases by each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13863746 ratings.\n",
      "+-------+----------+-----+\n",
      "|user_id|product_id|count|\n",
      "+-------+----------+-----+\n",
      "|  22352|     15873|    1|\n",
      "| 152610|     11175|    2|\n",
      "| 118860|     46979|   18|\n",
      "|   5430|     15424|   16|\n",
      "| 106387|     33768|    1|\n",
      "| 175918|     30850|    2|\n",
      "| 167393|     43692|    2|\n",
      "|  40286|     23001|    1|\n",
      "|   4076|     47049|    9|\n",
      "| 184598|     27086|    4|\n",
      "| 171199|     17896|    1|\n",
      "|  46527|     24497|    3|\n",
      "| 185432|     24852|   20|\n",
      "| 185432|     43014|   22|\n",
      "|  91926|     11182|    4|\n",
      "| 176010|     32134|   12|\n",
      "|  58707|     32373|    5|\n",
      "| 143707|     13176|   18|\n",
      "|  85359|     21137|   32|\n",
      "|  41502|     29487|    3|\n",
      "+-------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "implicit_ratings = order_products_df.join(orders_df, \n",
    "                                          order_products_df.order_id == orders_df.order_id).groupBy(\"user_id\", \n",
    "                                                                                                    \"product_id\").count()\n",
    "print(\"There are \" + str(implicit_ratings.count()) + \" ratings.\")\n",
    "implicit_ratings.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are over **13 million ratings**! The first 20 ratings are shown. Now lets train ALS model to find the best rank and select the one with the lowest RMSE to go in the final model. We will take a sample of the data to reduce computation time. We will first sample the data, the split into training, test, and evaluations sets. Because we are using implicit ratings, we will use the *trainImplicit* function from PySpark's mlib. Part of this code was inspired from __[here](https://www.codementor.io/jadianes/building-a-recommender-with-apache-spark-python-example-app-part1-du1083qbw)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 4.3713949667434555\n",
      "For rank 8 the RMSE is 4.3716101198856645\n",
      "For rank 12 the RMSE is 4.371843867487587\n",
      "The best model was trained with rank 4\n"
     ]
    }
   ],
   "source": [
    "# sample ratings\n",
    "sample_ratings = implicit_ratings.sample(False, 0.1).limit(11500000)\n",
    "\n",
    "# create train, test, and validation sets\n",
    "training, validation, test = sample_ratings.rdd.randomSplit([6, 2, 2])\n",
    "validation_prediction = validation.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# set parameters\n",
    "iterations = 20\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.trainImplicit(training, rank, seed=5, iterations=iterations)\n",
    "    predictions = model.predictAll(validation_prediction).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_predictions = validation.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_predictions.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print('For rank %s the RMSE is %s' % (rank, error))\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print('The best model was trained with rank %s' % best_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model had a rank of four we, will use this value to train the full model and make recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the **implicit ALS** model using a Rank of 4 and and 20 iterations. The model will be saved for future use such that our web application will only retrain on a needs basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train full model\n",
    "rank = 4\n",
    "numIterations = 20\n",
    "final_model = ALS.trainImplicit(implicit_ratings, rank, numIterations)\n",
    "final_model.save(sc, \"als_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will accept a user and the number of recommendations to make for that for that user. The recommendations will then me mapped to the product an returned. I wrote the function for easy integration into our web application to facilitate AJAX requests and that is why it returns a dictionary containing the status of the request and a list of products as data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make product recommendations to user\n",
    "def recommend(user_id, num_products):\n",
    "    # list to store recommended product IDs\n",
    "    product_ids=[]\n",
    "    # get recommendations as Ratings object\n",
    "    recommendations = final_model.recommendProducts(int(user_id), num_products)\n",
    "\n",
    "    # access Ratings object and extract product IDs\n",
    "    for rec in recommendations:\n",
    "        product_ids.append(rec.product)\n",
    "\n",
    "    # map product IDs to product an get aisle and department information\n",
    "    products = products_df.filter(products_df[\"product_id\"].isin(product_ids))\n",
    "    products = products.join(aisles_df,products_df.aisle_id == aisles_df.aisle_id)\n",
    "    products = products.join(departments_df, products_df.department_id == departments_df.department_id)\n",
    "    products = products.drop(\"aisle_id\",\"department_id\")\n",
    "\n",
    "    return {\"success\":True, \"webdata\":products.toPandas().values.tolist(), \"data\":products.toPandas()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  get 20 recommendations for user 25 and 678."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 25's Recommendations:\n",
      "product_id                   product_name                       aisle department\n",
      "      5876                  Organic Lemon                fresh fruits    produce\n",
      "      8277       Apple Honeycrisp Organic                fresh fruits    produce\n",
      "      8518              Organic Red Onion            fresh vegetables    produce\n",
      "     10749        Organic Red Bell Pepper            fresh vegetables    produce\n",
      "     13176         Bag of Organic Bananas                fresh fruits    produce\n",
      "     17794                        Carrots            fresh vegetables    produce\n",
      "     21137           Organic Strawberries                fresh fruits    produce\n",
      "     21903           Organic Baby Spinach  packaged vegetables fruits    produce\n",
      "     22935           Organic Yellow Onion            fresh vegetables    produce\n",
      "     24964                 Organic Garlic            fresh vegetables    produce\n",
      "     27104              Fresh Cauliflower            fresh vegetables    produce\n",
      "     27966            Organic Raspberries  packaged vegetables fruits    produce\n",
      "     30391               Organic Cucumber            fresh vegetables    produce\n",
      "     31717               Organic Cilantro                 fresh herbs    produce\n",
      "     34126  Organic Italian Parsley Bunch                 fresh herbs    produce\n",
      "     40706         Organic Grape Tomatoes  packaged vegetables fruits    produce\n",
      "     44359     Organic Small Bunch Celery            fresh vegetables    produce\n",
      "     45007               Organic Zucchini            fresh vegetables    produce\n",
      "     46667            Organic Ginger Root            fresh vegetables    produce\n",
      "     47209           Organic Hass Avocado                fresh fruits    produce\n",
      "\n",
      "\n",
      "\n",
      "User 678's Recommendations:\n",
      "product_id                               product_name                       aisle    department\n",
      "      4920                        Seedless Red Grapes  packaged vegetables fruits       produce\n",
      "      5077                     100% Whole Wheat Bread                       bread        bakery\n",
      "      9076                                Blueberries              frozen produce        frozen\n",
      "      9387                        Granny Smith Apples                fresh fruits       produce\n",
      "     10673  Original Nooks & Crannies English Muffins            breakfast bakery        bakery\n",
      "     13176                     Bag of Organic Bananas                fresh fruits       produce\n",
      "     16797                               Strawberries                fresh fruits       produce\n",
      "     16953                       Creamy Peanut Butter                     spreads        pantry\n",
      "     21137                       Organic Strawberries                fresh fruits       produce\n",
      "     23909                        2% Reduced Fat Milk                        milk    dairy eggs\n",
      "     24852                                     Banana                fresh fruits       produce\n",
      "     27796                            Real Mayonnaise                  condiments        pantry\n",
      "     27966                        Organic Raspberries  packaged vegetables fruits       produce\n",
      "     28204                         Organic Fuji Apple                fresh fruits       produce\n",
      "     33129                             Classic Hummus        fresh dips tapenades          deli\n",
      "     35042                                Black Beans          canned meals beans  canned goods\n",
      "     39275                        Organic Blueberries  packaged vegetables fruits       produce\n",
      "     41844                         Honey Nut Cheerios                      cereal     breakfast\n",
      "     43772            Cherubs Heavenly Salad Tomatoes            fresh vegetables       produce\n",
      "     47626                                Large Lemon                fresh fruits       produce\n"
     ]
    }
   ],
   "source": [
    "user25 = recommend(25, 20)\n",
    "print(\"User 25's Recommendations:\")\n",
    "print(user25['data'].to_string(index=False))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "user678 = recommend(678, 20)\n",
    "print(\"User 678's Recommendations:\")\n",
    "print(user678['data'].to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommendations seem quite interesting. It seems that user 25 is into healthly fruits and vegetables which is not surprizing in the current hype health and fitness awareness. User 678 on the other hand gets a fair share of fruits and vegetables but get products from several departments such as the dips from the deli and mayonnaise from the pantry. Now lets wrap this into a web application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommender above will be transformed into a web application using **Flask**. This is a fairly complex application for users that are new to web development because it relies on AJAX requests that allows the interface to be updated without refreshing the page. It uses **Javascript** produce the charts and uses JSON to serve **HTTP** requests. I created an **Object Oriented** class to serve the application from Python. The Python code is below. The Python code is only a portion of the application. It needs the **HTML, CSS, and JavaScript** files to run. It should be noted that I have used this HTML template in a group setting before and I have adopted it for this ALS recommender. You will need to research these topics if you are not familiar with them as the code below is just for demonstrative purposes. You can access the web application at http://35.170.73.218:5000/dashboard. \n",
    "All the code and other web files may be found at https://github.com/albert-gilharry/DATA-612/tree/master/Final%20Project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "DATA 612 Assignment Final Project: Instacart Recommender\n",
    "Authors: \n",
    "    Albert Gilharry\n",
    "\"\"\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import  SparkSession\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel\n",
    "from flask import Flask, render_template, request\n",
    "import json\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "class Recommender:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.department_dist = {\"departments\":[], \"orders\":[]}\n",
    "        #initialize Spark connection\n",
    "        # initialize a spark session\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "        conf = self.spark.sparkContext._conf.setAll([('spark.executor.memory', '20G'),\n",
    "                                                ('spark.app.name', 'Spark Updated Conf'), \n",
    "                                                ('spark.executor.cores', '4'), \n",
    "                                                ('spark.cores.max', '4'), \n",
    "                                                ('spark.driver.memory','20G'),\n",
    "                                                ('spark.executor.memoryOverhead', '20G')])\n",
    "        self.spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "        self.sc = self.spark.sparkContext\n",
    "        self.loadData()\n",
    "    \n",
    "    def loadData(self):\n",
    "        # check if data is already loaded into Spark\n",
    "        try:\n",
    "            self.aisles_df = self.spark.read.parquet(\"aisles.parquet\")\n",
    "            self.departments_df = self.spark.read.parquet(\"departments.parquet\")\n",
    "            self.order_products_df = self.spark.read.parquet(\"order_products.parquet\")\n",
    "            self.orders_df = self.spark.read.parquet(\"orders.parquet\")\n",
    "            self.products_df = self.spark.read.parquet(\"products.parquet\")\n",
    "    \n",
    "        except:\n",
    "            # if not, load data files\n",
    "            self.aisles_df = self.spark.read.format('csv').options(header='true', inferSchema='true').load('data/aisles.csv')\n",
    "            self.departments_df = self.spark.read.format('csv').options(header='true', inferSchema='true').load('data/departments.csv')\n",
    "            order_products = self.spark.read.format('csv').options(header='true', inferSchema='true').load('data/order_products__prior.csv')\n",
    "            self.order_products_df = order_products.union(self.spark.read.format('csv').options(header='true', inferSchema='true').load('data/order_products__train.csv'))\n",
    "            self.orders_df = self.spark.read.format('csv').options(header='true', inferSchema='true').load('data/orders.csv')\n",
    "            self.products_df = self.spark.read.format('csv').options(header='true', inferSchema='true').load('data/products.csv')\n",
    "            # persist data to Spark for future access\n",
    "            self.aisles_df.write.parquet(\"aisles.parquet\", mode = \"overwrite\" )\n",
    "            self.departments_df.write.parquet(\"departments.parquet\")\n",
    "            self.order_products_df.write.parquet(\"order_products.parquet\")\n",
    "            self.orders_df.write.parquet(\"orders.parquet\")\n",
    "            self.products_df.write.parquet(\"products.parquet\")\n",
    "        self.implicitRatings()\n",
    "            \n",
    "    def implicitRatings(self):\n",
    "        # create users table\n",
    "        self.users = self.orders_df.groupBy(\"user_id\").count()\n",
    "        self.users = self.users.drop(\"count\")\n",
    "        self.num_users = self.users.count()\n",
    "        # create ratings table\n",
    "        self.implicit_ratings = self.order_products_df.join(self.orders_df, self.order_products_df.order_id == self.orders_df.order_id).groupBy(\"user_id\", \"product_id\").count()\n",
    "        \n",
    "        # train ALS model if it is not available\n",
    "    def trainALS(self):\n",
    "        try:\n",
    "            self.model = MatrixFactorizationModel.load(self.sc, \"als_final\")\n",
    "        except (RuntimeError, TypeError, NameError) as e:\n",
    "            rank = 4\n",
    "            numIterations = 20\n",
    "            self.model = ALS.trainImplicit(self.implicit_ratings, rank, numIterations)\n",
    "            self.model.save(self.sc, \"als_final\")\n",
    "         \n",
    "         \n",
    "    # Get the recommended products to user    \n",
    "    def getRecommendations( self, user_id, n):\n",
    "        recommendedValue = self.item_similarity_top_k[self.item_similarity_top_k['user_id'] == user_id ]\n",
    "        return list(recommendedValue[\"item_id\"].astype(str))\n",
    "\t\n",
    "    # Create dashboard visuals\n",
    "    def getVisuals(self):\n",
    "        \n",
    "        # get popular aisles\n",
    "        aisles_dist = {\"success\":True,\"data\":[]}\n",
    "        \n",
    "        try:\n",
    "            self.aisles_distribution = self.spark.read.parquet(\"aisles_distribution.parquet\")\n",
    "        except:\n",
    "            aisles_distribution = self.implicit_ratings.join(self.products_df, self.implicit_ratings.product_id == self.products_df.product_id)\n",
    "            aisles_distribution=aisles_distribution.join(self.aisles_df,aisles_distribution.aisle_id == self.aisles_df.aisle_id).groupBy(\"aisle\").count()\n",
    "            aisles_distribution=aisles_distribution.sort(\"count\", ascending=False)\n",
    "            self.aisles_distribution=aisles_distribution.limit(10)\n",
    "            self.aisles_distribution.write.parquet(\"aisles_distribution.parquet\", mode = \"overwrite\")\n",
    "        aisles_dist['data'] = self.aisles_distribution.toPandas().values.tolist()\n",
    "        \n",
    "        # get orders by hour of day\n",
    "        hourly_orders = self.orders_df.groupBy(\"order_hour_of_day\").count()\n",
    "        hourly_orders = hourly_orders.sort(\"count\")\n",
    "        hour_dist = {\"success\":True,\"hour\":hourly_orders.select(\"order_hour_of_day\").toPandas().values.tolist(), \n",
    "                     \"orders\":hourly_orders.select(\"count\").toPandas().values.tolist()}\n",
    "        \n",
    "        # get orders by day of week\n",
    "        weekly_orders = self.orders_df.groupBy(\"order_dow\").count()\n",
    "        weekly_orders = weekly_orders.sort(\"count\")\n",
    "        weekly_orders_pd = weekly_orders.toPandas()\n",
    "        weekly_orders_pd = weekly_orders_pd.replace({0:\"Sunday\",1:\"Monday\",2:\"Tuesday\",3:\"Wednesday\",4:\"Thursday\",5:\"Friday\", 6:\"Saturday\"})\n",
    "        dow_dist = {\"success\":True, \"data\":weekly_orders_pd.values.tolist()}\n",
    "        \n",
    "        # other stats\n",
    "        num_orders = self.orders_df.count()\n",
    "        num_users = self.users.count()\n",
    "        num_products = self.products_df.count()\n",
    "        top_products = self.order_products_df.groupBy(\"product_id\").count()\n",
    "        top_products = top_products.sort(\"count\", ascending=False)\n",
    "        top_products = top_products.limit(1).join(self.products_df, top_products.product_id == self.products_df.product_id)\n",
    "        top_product=top_products.collect()[0]['product_name']\n",
    "        \n",
    "        return {\"aisles\":aisles_dist,\"doweek\":dow_dist,\"hour_of_day\":hour_dist,\n",
    "                \"num_orders\":num_orders, \n",
    "                \"num_users\":num_users,\n",
    "                \"num_products\":num_products,\n",
    "                \"top_product\":top_product}\n",
    "        \n",
    "    # Get a sample of users to reduce load on the interface\n",
    "    def sampleUsers(self):\n",
    "        self.users.sample(False, 0.1).limit(200).toPandas().values.tolist()\n",
    "        return {\"success\":True, \"data\": self.users.sample(False, 0.1).limit(200).toPandas().values.tolist()}\n",
    "    \n",
    "    # Send recommendations along with additonal product information to the browswer \n",
    "    def recommend(self, user_id):\n",
    "        self.trainALS()\n",
    "        product_ids=[]\n",
    "        recommendations = self.model.recommendProducts(int(user_id), 20)\n",
    "        print(recommendations)\n",
    "        for rec in recommendations:\n",
    "            product_ids.append(rec.product)\n",
    "            print(int(rec.product))\n",
    "        print(product_ids)\n",
    "        products = self.products_df.filter(self.products_df[\"product_id\"].isin(product_ids))\n",
    "        products = products.join(self.aisles_df,self.products_df.aisle_id == self.aisles_df.aisle_id)\n",
    "        products = products.join(self.departments_df, self.products_df.department_id == self.departments_df.department_id)\n",
    "        products = products.drop(\"aisle_id\",\"department_id\")\n",
    "        \n",
    "        return {\"success\":True, \"data\":products.toPandas().values.tolist()}\n",
    "\n",
    "recommender = Recommender()\n",
    "\n",
    "@app.route(\"/\")\n",
    "def main():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/dashboard\")\n",
    "def dashboard():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route(\"/recommendations\")\n",
    "def recommendations():\n",
    "    return render_template('recommendations.html')\n",
    "\n",
    "@app.route(\"/getGraphics\",methods=['GET'])\n",
    "def getGraphics():\n",
    "    graphics = recommender.getVisuals()\n",
    "    return json.dumps(graphics) \n",
    "\n",
    "@app.route(\"/sampleUsers\",methods=['GET'])\n",
    "def sampleUsers():\n",
    "    sample = recommender.sampleUsers()\n",
    "    return json.dumps(sample) \n",
    "\n",
    "@app.route(\"/getRecommendations\",methods=['POST'])\n",
    "def getRecommendations():\n",
    "    user_id = request.form['user']\n",
    "    recommendations = recommender.recommend(user_id)\n",
    "    return json.dumps(recommendations) \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
